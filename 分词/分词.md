## Tokenization
- Token
  In the modern deep learning area,Token means the basic unit that makes up text,it can be a single world,a character,a sentence or even a subword.For example,a sentence like "i love reading" can be tokenized into ["i","love","reading"],at this sentence,each word can be regarded as a Token.</p>
  To convert text into a format that machines can understand, we need to use Tokenizer to perform this conversion.**Tokenizer** is a tool or algorithm to split text into series tokens.Next i will introduce several tokenizer method.
  - **Word level**

<div style="padding: 10px; background-color: #f0f8fb; border-left: 4px solid #4299e1; border-radius: 4px;">
  <strong>ℹ️ 信息：</strong> 这是一般性信息提示
</div>